{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordlevelmodel.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "EpfCDvru1WWy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib.request as request\n",
        "from pathlib import Path\n",
        "from pickle import dump\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "page = request.urlopen('https://raw.githubusercontent.com/ShreyaGupta08/Lyrics-Generator/master/data/data.txt?token=AVvj6Rofb-Azo9OmR2gHhIu2y0yNH3Qkks5cpKPkwA%3D%3D')\n",
        "doc = bytes(page.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "5qexz_6D1WW3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words = doc.split()\n",
        "words = words[ : int(0.1*len(words))]\n",
        "vocab_size = len(set(words))\n",
        "dic = {}\n",
        "rev = {}\n",
        "cnt = 0\n",
        "for ix in set(words):\n",
        "    dic[ix] = cnt\n",
        "    rev[cnt] = ix\n",
        "    cnt += 1\n",
        "sequence = []\n",
        "for ix in words:\n",
        "    sequence.append(dic[ix])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "IK5AinH31WW6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "seq_len = 5\n",
        "X = []\n",
        "y = []\n",
        "for ix in range(0, len(sequence)-seq_len, seq_len):\n",
        "    X.append(sequence[ix:ix+seq_len])\n",
        "    y.append(sequence[ix+seq_len])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3kK7ZNRX1WW9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq_length = X.shape[1]\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "JkaldmHJ1WW_",
        "colab_type": "code",
        "outputId": "3ce11c61-5363-4323-ec29-5ce42f051457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "cell_type": "code",
      "source": [
        "# define model\n",
        "vec_size = 10\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, vec_size, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 5, 10)             101600    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 5, 100)            44400     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10160)             1026160   \n",
            "=================================================================\n",
            "Total params: 1,262,660\n",
            "Trainable params: 1,262,660\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "j1cGyAPG1WXE",
        "colab_type": "code",
        "outputId": "e28f7b20-25db-429c-eed5-798ced90f642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7077
        }
      },
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=200)\n",
        "model.save('ep300.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "108769/108769 [==============================] - 36s 331us/step - loss: 6.6338 - acc: 0.0396\n",
            "Epoch 2/200\n",
            "108769/108769 [==============================] - 34s 317us/step - loss: 6.3011 - acc: 0.0402\n",
            "Epoch 3/200\n",
            "108769/108769 [==============================] - 34s 310us/step - loss: 6.0937 - acc: 0.0489\n",
            "Epoch 4/200\n",
            "108769/108769 [==============================] - 33s 307us/step - loss: 5.8955 - acc: 0.0641\n",
            "Epoch 5/200\n",
            "108769/108769 [==============================] - 34s 309us/step - loss: 5.7411 - acc: 0.0737\n",
            "Epoch 6/200\n",
            "108769/108769 [==============================] - 33s 306us/step - loss: 5.6052 - acc: 0.0795\n",
            "Epoch 7/200\n",
            "108769/108769 [==============================] - 33s 307us/step - loss: 5.4825 - acc: 0.0842\n",
            "Epoch 8/200\n",
            "108769/108769 [==============================] - 33s 307us/step - loss: 5.3650 - acc: 0.0904\n",
            "Epoch 9/200\n",
            "108769/108769 [==============================] - 33s 307us/step - loss: 5.2426 - acc: 0.0969\n",
            "Epoch 10/200\n",
            "108769/108769 [==============================] - 33s 306us/step - loss: 5.1158 - acc: 0.1046\n",
            "Epoch 11/200\n",
            "108769/108769 [==============================] - 33s 307us/step - loss: 4.9865 - acc: 0.1137\n",
            "Epoch 12/200\n",
            "108769/108769 [==============================] - 33s 307us/step - loss: 4.8560 - acc: 0.1206\n",
            "Epoch 13/200\n",
            "108769/108769 [==============================] - 33s 305us/step - loss: 4.7259 - acc: 0.1283\n",
            "Epoch 14/200\n",
            "108769/108769 [==============================] - 33s 308us/step - loss: 4.5961 - acc: 0.1367\n",
            "Epoch 15/200\n",
            "108769/108769 [==============================] - 33s 306us/step - loss: 4.4690 - acc: 0.1456\n",
            "Epoch 16/200\n",
            "108769/108769 [==============================] - 34s 309us/step - loss: 4.3463 - acc: 0.1572\n",
            "Epoch 17/200\n",
            "108769/108769 [==============================] - 34s 309us/step - loss: 4.2329 - acc: 0.1691\n",
            "Epoch 18/200\n",
            "108769/108769 [==============================] - 33s 307us/step - loss: 4.1254 - acc: 0.1828\n",
            "Epoch 19/200\n",
            "108769/108769 [==============================] - 33s 305us/step - loss: 4.0252 - acc: 0.1956\n",
            "Epoch 20/200\n",
            "108769/108769 [==============================] - 33s 305us/step - loss: 3.9363 - acc: 0.2087\n",
            "Epoch 21/200\n",
            "108769/108769 [==============================] - 33s 305us/step - loss: 3.8538 - acc: 0.2205\n",
            "Epoch 22/200\n",
            "108769/108769 [==============================] - 33s 304us/step - loss: 3.7777 - acc: 0.2307\n",
            "Epoch 23/200\n",
            "108769/108769 [==============================] - 33s 303us/step - loss: 3.7053 - acc: 0.2433\n",
            "Epoch 24/200\n",
            "108769/108769 [==============================] - 33s 301us/step - loss: 3.6402 - acc: 0.2528\n",
            "Epoch 25/200\n",
            "108769/108769 [==============================] - 33s 302us/step - loss: 3.5803 - acc: 0.2624\n",
            "Epoch 26/200\n",
            "108769/108769 [==============================] - 33s 302us/step - loss: 3.5226 - acc: 0.2705\n",
            "Epoch 27/200\n",
            "108769/108769 [==============================] - 33s 303us/step - loss: 3.4673 - acc: 0.2812\n",
            "Epoch 28/200\n",
            "108769/108769 [==============================] - 33s 303us/step - loss: 3.4157 - acc: 0.2881\n",
            "Epoch 29/200\n",
            "108769/108769 [==============================] - 33s 303us/step - loss: 3.3666 - acc: 0.2971\n",
            "Epoch 30/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 3.3201 - acc: 0.3044\n",
            "Epoch 31/200\n",
            "108769/108769 [==============================] - 33s 301us/step - loss: 3.2757 - acc: 0.3123\n",
            "Epoch 32/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 3.2335 - acc: 0.3181\n",
            "Epoch 33/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 3.1898 - acc: 0.3261\n",
            "Epoch 34/200\n",
            "108769/108769 [==============================] - 33s 301us/step - loss: 3.1519 - acc: 0.3332\n",
            "Epoch 35/200\n",
            "108769/108769 [==============================] - 33s 302us/step - loss: 3.1139 - acc: 0.3394\n",
            "Epoch 36/200\n",
            "108769/108769 [==============================] - 33s 302us/step - loss: 3.0774 - acc: 0.3459\n",
            "Epoch 37/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 3.0422 - acc: 0.3512\n",
            "Epoch 38/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 3.0065 - acc: 0.3581\n",
            "Epoch 39/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 2.9733 - acc: 0.3622\n",
            "Epoch 40/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 2.9407 - acc: 0.3693\n",
            "Epoch 41/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.9098 - acc: 0.3744\n",
            "Epoch 42/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.8801 - acc: 0.3792\n",
            "Epoch 43/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 2.8485 - acc: 0.3855\n",
            "Epoch 44/200\n",
            "108769/108769 [==============================] - 33s 301us/step - loss: 2.8220 - acc: 0.3903\n",
            "Epoch 45/200\n",
            "108769/108769 [==============================] - 33s 301us/step - loss: 2.7897 - acc: 0.3955\n",
            "Epoch 46/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 2.7643 - acc: 0.4012\n",
            "Epoch 47/200\n",
            "108769/108769 [==============================] - 33s 301us/step - loss: 2.7348 - acc: 0.4052\n",
            "Epoch 48/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 2.7113 - acc: 0.4095\n",
            "Epoch 49/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 2.6851 - acc: 0.4137\n",
            "Epoch 50/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.6572 - acc: 0.4203\n",
            "Epoch 51/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.6357 - acc: 0.4240\n",
            "Epoch 52/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.6101 - acc: 0.4276\n",
            "Epoch 53/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.5855 - acc: 0.4330\n",
            "Epoch 54/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.5650 - acc: 0.4363\n",
            "Epoch 55/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.5419 - acc: 0.4406\n",
            "Epoch 56/200\n",
            "108769/108769 [==============================] - 32s 299us/step - loss: 2.5196 - acc: 0.4435\n",
            "Epoch 57/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.4961 - acc: 0.4484\n",
            "Epoch 58/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 2.4780 - acc: 0.4516\n",
            "Epoch 59/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.4559 - acc: 0.4561\n",
            "Epoch 60/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.4348 - acc: 0.4602\n",
            "Epoch 61/200\n",
            "108769/108769 [==============================] - 32s 299us/step - loss: 2.4139 - acc: 0.4619\n",
            "Epoch 62/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.3953 - acc: 0.4665\n",
            "Epoch 63/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.3753 - acc: 0.4701\n",
            "Epoch 64/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.3564 - acc: 0.4737\n",
            "Epoch 65/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.3400 - acc: 0.4761\n",
            "Epoch 66/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.3214 - acc: 0.4807\n",
            "Epoch 67/200\n",
            "108769/108769 [==============================] - 32s 299us/step - loss: 2.3002 - acc: 0.4836\n",
            "Epoch 68/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.2867 - acc: 0.4860\n",
            "Epoch 69/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.2661 - acc: 0.4912\n",
            "Epoch 70/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.2528 - acc: 0.4920\n",
            "Epoch 71/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 2.2334 - acc: 0.4962\n",
            "Epoch 72/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 2.2143 - acc: 0.4999\n",
            "Epoch 73/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.2020 - acc: 0.5032\n",
            "Epoch 74/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.1848 - acc: 0.5048\n",
            "Epoch 75/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.1701 - acc: 0.5084\n",
            "Epoch 76/200\n",
            "108769/108769 [==============================] - 33s 300us/step - loss: 2.1558 - acc: 0.5107\n",
            "Epoch 77/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.1388 - acc: 0.5132\n",
            "Epoch 78/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.1232 - acc: 0.5158\n",
            "Epoch 79/200\n",
            "108769/108769 [==============================] - 33s 299us/step - loss: 2.1114 - acc: 0.5181\n",
            "Epoch 80/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.0944 - acc: 0.5223\n",
            "Epoch 81/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.0819 - acc: 0.5252\n",
            "Epoch 82/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.0714 - acc: 0.5263\n",
            "Epoch 83/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.0570 - acc: 0.5287\n",
            "Epoch 84/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.0409 - acc: 0.5319\n",
            "Epoch 85/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.0269 - acc: 0.5344\n",
            "Epoch 86/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.0129 - acc: 0.5378\n",
            "Epoch 87/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 2.0024 - acc: 0.5385\n",
            "Epoch 88/200\n",
            "108769/108769 [==============================] - 32s 298us/step - loss: 1.9907 - acc: 0.5411\n",
            "Epoch 89/200\n",
            "108769/108769 [==============================] - 32s 297us/step - loss: 1.9813 - acc: 0.5414\n",
            "Epoch 90/200\n",
            "108769/108769 [==============================] - 32s 299us/step - loss: 1.9641 - acc: 0.5463\n",
            "Epoch 91/200\n",
            "108769/108769 [==============================] - 32s 296us/step - loss: 1.9550 - acc: 0.5478\n",
            "Epoch 92/200\n",
            "108769/108769 [==============================] - 32s 294us/step - loss: 1.9358 - acc: 0.5528\n",
            "Epoch 93/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.9319 - acc: 0.5523\n",
            "Epoch 94/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.9190 - acc: 0.5543\n",
            "Epoch 95/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.9089 - acc: 0.5573\n",
            "Epoch 96/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.8910 - acc: 0.5610\n",
            "Epoch 97/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.8831 - acc: 0.5610\n",
            "Epoch 98/200\n",
            "108769/108769 [==============================] - 32s 296us/step - loss: 1.8726 - acc: 0.5638\n",
            "Epoch 99/200\n",
            "108769/108769 [==============================] - 32s 294us/step - loss: 1.8613 - acc: 0.5666\n",
            "Epoch 100/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.8528 - acc: 0.5676\n",
            "Epoch 101/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.8408 - acc: 0.5692\n",
            "Epoch 102/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.8284 - acc: 0.5710\n",
            "Epoch 103/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.8196 - acc: 0.5742\n",
            "Epoch 104/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.8123 - acc: 0.5752\n",
            "Epoch 105/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.8008 - acc: 0.5776\n",
            "Epoch 106/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.7892 - acc: 0.5797\n",
            "Epoch 107/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.7841 - acc: 0.5809\n",
            "Epoch 108/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.7703 - acc: 0.5825\n",
            "Epoch 109/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.7602 - acc: 0.5849\n",
            "Epoch 110/200\n",
            "108769/108769 [==============================] - 32s 291us/step - loss: 1.7565 - acc: 0.5858\n",
            "Epoch 111/200\n",
            "108769/108769 [==============================] - 32s 291us/step - loss: 1.7412 - acc: 0.5875\n",
            "Epoch 112/200\n",
            "108769/108769 [==============================] - 32s 291us/step - loss: 1.7338 - acc: 0.5910\n",
            "Epoch 113/200\n",
            "108769/108769 [==============================] - 32s 290us/step - loss: 1.7267 - acc: 0.5907\n",
            "Epoch 114/200\n",
            "108769/108769 [==============================] - 32s 291us/step - loss: 1.7175 - acc: 0.5932\n",
            "Epoch 115/200\n",
            "108769/108769 [==============================] - 32s 290us/step - loss: 1.7111 - acc: 0.5943\n",
            "Epoch 116/200\n",
            "108769/108769 [==============================] - 32s 294us/step - loss: 1.6979 - acc: 0.5978\n",
            "Epoch 117/200\n",
            "108769/108769 [==============================] - 32s 294us/step - loss: 1.6861 - acc: 0.6005\n",
            "Epoch 118/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.6847 - acc: 0.5997\n",
            "Epoch 119/200\n",
            "108769/108769 [==============================] - 32s 291us/step - loss: 1.6752 - acc: 0.6017\n",
            "Epoch 120/200\n",
            "108769/108769 [==============================] - 32s 291us/step - loss: 1.6664 - acc: 0.6037\n",
            "Epoch 121/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.6585 - acc: 0.6056\n",
            "Epoch 122/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.6505 - acc: 0.6068\n",
            "Epoch 123/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.6445 - acc: 0.6081\n",
            "Epoch 124/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.6329 - acc: 0.6090\n",
            "Epoch 125/200\n",
            "108769/108769 [==============================] - 32s 296us/step - loss: 1.6271 - acc: 0.6112\n",
            "Epoch 126/200\n",
            "108769/108769 [==============================] - 32s 297us/step - loss: 1.6183 - acc: 0.6132\n",
            "Epoch 127/200\n",
            "108769/108769 [==============================] - 32s 297us/step - loss: 1.6063 - acc: 0.6159\n",
            "Epoch 128/200\n",
            "108769/108769 [==============================] - 32s 296us/step - loss: 1.6066 - acc: 0.6167\n",
            "Epoch 129/200\n",
            "108769/108769 [==============================] - 32s 297us/step - loss: 1.5954 - acc: 0.6171\n",
            "Epoch 130/200\n",
            "108769/108769 [==============================] - 32s 296us/step - loss: 1.5858 - acc: 0.6200\n",
            "Epoch 131/200\n",
            "108769/108769 [==============================] - 32s 296us/step - loss: 1.5831 - acc: 0.6206\n",
            "Epoch 132/200\n",
            "108769/108769 [==============================] - 32s 295us/step - loss: 1.5687 - acc: 0.6238\n",
            "Epoch 133/200\n",
            "108769/108769 [==============================] - 32s 295us/step - loss: 1.5715 - acc: 0.6230\n",
            "Epoch 134/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.5619 - acc: 0.6241\n",
            "Epoch 135/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.5548 - acc: 0.6266\n",
            "Epoch 136/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.5467 - acc: 0.6279\n",
            "Epoch 137/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.5446 - acc: 0.6276\n",
            "Epoch 138/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.5348 - acc: 0.6298\n",
            "Epoch 139/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.5233 - acc: 0.6322\n",
            "Epoch 140/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.5233 - acc: 0.6309\n",
            "Epoch 141/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.5161 - acc: 0.6335\n",
            "Epoch 142/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.5077 - acc: 0.6361\n",
            "Epoch 143/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.5019 - acc: 0.6372\n",
            "Epoch 144/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.4931 - acc: 0.6376\n",
            "Epoch 145/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.4945 - acc: 0.6373\n",
            "Epoch 146/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.4881 - acc: 0.6384\n",
            "Epoch 147/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.4724 - acc: 0.6442\n",
            "Epoch 148/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.4764 - acc: 0.6410\n",
            "Epoch 149/200\n",
            "108769/108769 [==============================] - 32s 291us/step - loss: 1.4637 - acc: 0.6442\n",
            "Epoch 150/200\n",
            "108769/108769 [==============================] - 32s 291us/step - loss: 1.4591 - acc: 0.6460\n",
            "Epoch 151/200\n",
            "108769/108769 [==============================] - 32s 291us/step - loss: 1.4556 - acc: 0.6462\n",
            "Epoch 152/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.4476 - acc: 0.6473\n",
            "Epoch 153/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.4424 - acc: 0.6488\n",
            "Epoch 154/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.4349 - acc: 0.6500\n",
            "Epoch 155/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.4393 - acc: 0.6482\n",
            "Epoch 156/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.4281 - acc: 0.6517\n",
            "Epoch 157/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.4193 - acc: 0.6533\n",
            "Epoch 158/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.4227 - acc: 0.6515\n",
            "Epoch 159/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.4064 - acc: 0.6563\n",
            "Epoch 160/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3999 - acc: 0.6586\n",
            "Epoch 161/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3989 - acc: 0.6567\n",
            "Epoch 162/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3990 - acc: 0.6573\n",
            "Epoch 163/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3893 - acc: 0.6591\n",
            "Epoch 164/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3827 - acc: 0.6613\n",
            "Epoch 165/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3763 - acc: 0.6627\n",
            "Epoch 166/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3782 - acc: 0.6614\n",
            "Epoch 167/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.3715 - acc: 0.6641\n",
            "Epoch 168/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.3658 - acc: 0.6647\n",
            "Epoch 169/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3618 - acc: 0.6651\n",
            "Epoch 170/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.3557 - acc: 0.6669\n",
            "Epoch 171/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.3486 - acc: 0.6687\n",
            "Epoch 172/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.3477 - acc: 0.6687\n",
            "Epoch 173/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3429 - acc: 0.6689\n",
            "Epoch 174/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3320 - acc: 0.6716\n",
            "Epoch 175/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.3347 - acc: 0.6693\n",
            "Epoch 176/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3305 - acc: 0.6722\n",
            "Epoch 177/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3245 - acc: 0.6728\n",
            "Epoch 178/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.3185 - acc: 0.6743\n",
            "Epoch 179/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3165 - acc: 0.6745\n",
            "Epoch 180/200\n",
            "108769/108769 [==============================] - 32s 291us/step - loss: 1.3044 - acc: 0.6779\n",
            "Epoch 181/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3042 - acc: 0.6775\n",
            "Epoch 182/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.3044 - acc: 0.6766\n",
            "Epoch 183/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.2988 - acc: 0.6775\n",
            "Epoch 184/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.2928 - acc: 0.6797\n",
            "Epoch 185/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.2811 - acc: 0.6843\n",
            "Epoch 186/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.2919 - acc: 0.6801\n",
            "Epoch 187/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.2785 - acc: 0.6829\n",
            "Epoch 188/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.2727 - acc: 0.6848\n",
            "Epoch 189/200\n",
            "108769/108769 [==============================] - 32s 294us/step - loss: 1.2729 - acc: 0.6835\n",
            "Epoch 190/200\n",
            "108769/108769 [==============================] - 32s 294us/step - loss: 1.2680 - acc: 0.6851\n",
            "Epoch 191/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.2629 - acc: 0.6873\n",
            "Epoch 192/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.2590 - acc: 0.6868\n",
            "Epoch 193/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.2637 - acc: 0.6845\n",
            "Epoch 194/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.2567 - acc: 0.6863\n",
            "Epoch 195/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.2416 - acc: 0.6917\n",
            "Epoch 196/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.2452 - acc: 0.6906\n",
            "Epoch 197/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.2434 - acc: 0.6906\n",
            "Epoch 198/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.2338 - acc: 0.6922\n",
            "Epoch 199/200\n",
            "108769/108769 [==============================] - 32s 292us/step - loss: 1.2276 - acc: 0.6945\n",
            "Epoch 200/200\n",
            "108769/108769 [==============================] - 32s 293us/step - loss: 1.2379 - acc: 0.6913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3ELHMK4_1WXK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inp = words[2000:2000+seq_len]\n",
        "X_test = []\n",
        "for ix in inp: \n",
        "    X_test.append(dic[ix])\n",
        "X_test = np.array(X_test)\n",
        "X_test.shape\n",
        "X_test = X_test.reshape(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "n1RJMLU_1WXN",
        "colab_type": "code",
        "outputId": "3d3d1379-97c2-4c5f-d127-678fee74f68f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "seq = []\n",
        "for ix in range(X_test.shape[1]):\n",
        "    seq.append(X_test[0, ix])\n",
        "len(seq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tVehIojN1WXP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for ix in range(0, 10):\n",
        "    pred = model.predict(X_test)\n",
        "    seq.append(int(np.argmax(pred)))\n",
        "    X_test = np.array(seq[ix+1:]).reshape(1, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "mxT7VZnR1WXR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "song = []\n",
        "for ix in range(len(seq)):\n",
        "    song.append(rev[seq[ix]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "nrF_e-3Q1WXU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "c-pfdcy01WXW",
        "colab_type": "code",
        "outputId": "270148a0-2be1-4983-a29f-353dfd32a77d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "cell_type": "code",
      "source": [
        "for ix in song:\n",
        "  print(ix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'for'\n",
            "b'you'\n",
            "b'the'\n",
            "b'one'\n",
            "b'that'\n",
            "b'i'\n",
            "b'adore'\n",
            "b'hold'\n",
            "b'by'\n",
            "b'a'\n",
            "b'business'\n",
            "b'man'\n",
            "b'youve'\n",
            "b'known'\n",
            "b'me'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VgMKdPlaZ9Bu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b83TuLpJnqv5",
        "colab_type": "code",
        "outputId": "9b9f8456-2e2d-4d14-898a-4d072bc7f3cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "final_lyrics=''\n",
        "for x in song:\n",
        "  final_lyrics = final_lyrics + ' ' + str(x,'utf-8')\n",
        "  \n",
        "final_lyrics"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' for you the one that i adore hold by a business man youve known me'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "ATYtMlO5rXxC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}